import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import requests
import json

class OllamaChat():
    def __init__(self, system_message="ä½ ç°åœ¨æ˜¯ä¸€ä¸ªå››è½®å°è½¦åº•åº§+å…­è‡ªç”±åº¦å¤¹çˆªæœºæ¢°è‡‚çš„å¤§è„‘ï¼Œè¿™ä¸€ç³»ç»Ÿå…·å¤‡æœ‰è§†è§‰åŠŸèƒ½ï¼Œè¯­éŸ³è¯†åˆ«ä¸è¾“å‡ºåŠŸèƒ½ã€æœºæ¢°è‡‚æ§åˆ¶åŠŸèƒ½ï¼Œä½ çš„ä¸»è¦ä»»åŠ¡æ˜¯å……å½“æŒ‡ä»¤åˆ°åˆ†è§£çš„ä»»åŠ¡çš„è½¬æ¢å‘˜ï¼Œå°†ä»è¯­éŸ³ç³»ç»Ÿè½¬æ¢çš„æŒ‡ä»¤è½¬åŒ–ä¸ºå„ä¸ªå­ç³»ç»Ÿçš„ç»†åˆ†ä»»åŠ¡ã€‚",
                 url="http://162.105.175.7:11434/api/chat", model_name="deepseek-r1:14b"):
        self.url = url
        self.model_name = model_name
        self.system_message = {
            "role": "system",
            "content": f"{system_message}"
        }

    def ouput_response(self, response, stream=False, is_chat=True):
        if stream:
            return_text = ''
            for chunk in response.iter_content(chunk_size=None):
                if chunk:
                    if is_chat:
                        text = json.loads(chunk.decode('utf-8'))['message']['content']
                    else:
                        text = json.loads(chunk.decode('utf-8'))['response']
                    return_text += text
                    print(text, end='', flush=True)
        else:
            if is_chat:
                return_text = ''.join([
                    json.loads(line)['message']['content']
                    for line in response.text.split('\n') if line.strip()
                ])
            else:
                return_text = ''.join([
                    json.loads(line)['response']
                    for line in response.text.split('\n') if line.strip()
                ])
        return return_text

    def chat(self, prompt, message=[], stream=False, temperature=None):
        if not message:
            message.append(self.system_message)
        message.append({"role": "user", "content": prompt + '. å¦‚æœå¯¹è¯æ˜¯ä¸€æ¡æŒ‡ä»¤ï¼Œè¯·ä½ ä»…ä»¥jsonæ ¼å¼è¿”å›, jsonç±»ä¼¼{"task": "æŠ“å–", "object": "çº¢è‰²æ¯å­", "location": "table"â€¦â€¦}'})
        data = {
            "model": self.model_name,
            "messages": message 
        }
        if temperature is not None:
            data["options"] = {"temperature": temperature}
        headers = {"Content-Type": "application/json"}
        responses = requests.post(self.url, headers=headers, json=data, stream=stream)
        return_text = self.ouput_response(responses, stream)
        message.append({"role": "assistant", "content": return_text})
        return return_text, message

class LLMNode(Node):
    def __init__(self):
        super().__init__('llm_node')
        self.sub = self.create_subscription(String, 'voice_command', self.cb, 10)
        self.pub = self.create_publisher(String, 'parsed_plan', 10)
        self.chatbot = OllamaChat()
        self.msg_history = []
        self.get_logger().info("ğŸ§  LLM Node Ready - ä½¿ç”¨ DeepSeek-R1-14B")

    def cb(self, msg: String):
        prompt = msg.data.strip()
        self.get_logger().info(f"ğŸ§  æ¥æ”¶åˆ°è¯­éŸ³æŒ‡ä»¤: {prompt}")
        try:
            answer, self.msg_history = self.chatbot.chat(prompt, self.msg_history, stream=False)
            plan_msg = String()
            plan_msg.data = answer
            self.pub.publish(plan_msg)
            self.get_logger().info(f"ğŸ“¤ å‘å¸ƒè§£æç»“æœ: {answer}")
        except Exception as e:
            self.get_logger().error(f"âŒ è°ƒç”¨ LLM å¤±è´¥: {e}")

def main(args=None):
    rclpy.init(args=args)
    rclpy.spin(LLMNode())
    rclpy.shutdown()
