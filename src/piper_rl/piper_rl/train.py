import os
import numpy as np
import gym
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from piper_rl_node import MyRobotEnv
import time


def train():
    env = MyRobotEnv()
    model = PPO(
        policy="MlpPolicy",
        env=env,
        policy_kwargs=dict(net_arch=[256, 128]),
        learning_rate=1e-3,
        batch_size=128,
        n_steps=1024,
        gamma=0.98,
        verbose=1,
        ent_coef=0.05,
        tensorboard_log="./ppo_logs/",
    )
    model.set_parameters("./ppo_models/piper_rl_100000_steps")

    checkpoint_callback = CheckpointCallback(
        save_freq=50000, save_path="./ppo_models/", name_prefix="piper_rl"
    )

    model.learn(total_timesteps=200000 * 5, callback=checkpoint_callback)

    model.save("ppo_piper_final")
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜ä¸º ppo_piper_final.zip")


def test():
    env = MyRobotEnv()
    model = PPO.load("ppo_piper_final_maximize_z")
    obs = env.reset()
    for epoch in range(1000):
        # time.sleep(1)
        action, _ = model.predict(obs)
        obs, reward, done, _ = env.step(action)

        print(f"epoch {epoch}, Reward:", reward)
        if done:
            print("ğŸ‰ æˆåŠŸæŠ“å–ï¼Œé‡æ–°å¼€å§‹")
            obs = env.reset()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•è®­ç»ƒå¥½çš„ç­–ç•¥")
    args = parser.parse_args()

    if args.test:
        test()
    else:
        train()
