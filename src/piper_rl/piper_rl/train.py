import os
import numpy as np
from gymnasium.wrappers import RecordVideo
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
import time


def decay_schedule(initial_value):
    def func(progress_remaining):
        return initial_value * (progress_remaining)

    return func


def make_env(MyRobotEnv, worker_id):
    def _init():
        env = MyRobotEnv(worker_id=worker_id)
        return env

    return _init


def train():
    if args.gazebo:
        env = GazeboRobotEnv()
    else:
        if args.proc > 1:
            from stable_baselines3.common.vec_env import SubprocVecEnv

            env = SubprocVecEnv(
                [make_env(MujocoRobotEnv, i) for i in range(args.proc)],
                start_method="spawn",
            )
        else:
            # éœ€è¦ export MUJOCO_GL=egl
            env = MujocoRobotEnv(render_mode="rgb_array")
            # å¤šè¿›ç¨‹ä»¿çœŸä¸æ”¯æŒå½•åˆ¶
            if args.train_record:
                video_dir = "./videos/"
                env = RecordVideo(
                    env,
                    video_folder=video_dir,
                    episode_trigger=lambda e: e % 100 == 0,
                    video_length=5000,
                )
    model = PPO(
        policy="MlpPolicy",
        env=env,
        device="cpu",
        policy_kwargs=dict(
            net_arch=[256, 512, 256, 128],
            log_std_init=-2.0,
            ortho_init=True,
        ),
        # åœ¨å·²è®­ç»ƒå¥½çš„æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒï¼Œéœ€è¦è°ƒå°å­¦ä¹ ç‡ï¼Œå¦‚3e-5
        learning_rate=decay_schedule(3e-4),
        batch_size=1024,
        n_steps=2048,
        gamma=0.99,
        verbose=1,
        # è¶Šé«˜è¶Šé¼“åŠ±æ¢ç´¢ï¼Œä½†ä¸èƒ½å¤ªé«˜ï¼Œå¦åˆ™stdä¼šå˜å¤§ï¼Œç­–ç•¥ä¼šå˜å¾—ä¸ç¨³å®šï¼Œä¸€èˆ¬1e-2ä»¥å†…
        ent_coef=1e-2,
        tensorboard_log="./ppo_logs/",
    )
    # model.set_parameters("ppo_piper_final_best.zip")

    checkpoint_callback = CheckpointCallback(
        save_freq=100000, save_path="./ppo_models/", name_prefix="piper_rl_checkpoint"
    )

    model.learn(total_timesteps=1000 * 1000 * 100, callback=checkpoint_callback)

    model.save("ppo_piper_final")
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜ä¸º ppo_piper_final.zip")


def test():
    if args.gazebo:
        env = GazeboRobotEnv()
    else:
        env = MujocoRobotEnv()
        video_dir = "./videos/"
        env = RecordVideo(
            env,
            video_folder=video_dir,
            episode_trigger=lambda e: e % 100 == 0,
            video_length=5000,
        )
    model = PPO.load("ppo_piper_final_maximize_z")
    obs = env.reset()
    for epoch in range(100000):
        # time.sleep(1)
        action, _ = model.predict(obs)
        obs, reward, done, _ = env.step(action)

        # print(f"epoch {epoch}, Reward:", reward)
        if done:
            print("ğŸ‰ æˆåŠŸæŠ“å–ï¼Œé‡æ–°å¼€å§‹")
            obs = env.reset()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•è®­ç»ƒå¥½çš„ç­–ç•¥")
    parser.add_argument("--gazebo", action="store_true", help="ä½¿ç”¨mujocoä»¿çœŸ")
    parser.add_argument(
        "--train_record",
        default=True,
        action="store_true",
        help="è®­ç»ƒæ—¶å®šæ—¶å½•åˆ¶è®­ç»ƒè¿‡ç¨‹",
    )
    parser.add_argument("--proc", default=64, help="å¹¶è¡Œä»¿çœŸè¿›ç¨‹æ•°")
    args = parser.parse_args()
    if args.gazebo:
        from piper_rl_gazebo_node import GazeboRobotEnv

        print("ğŸš€ ä½¿ç”¨Gazeboä»¿çœŸ")
    else:
        from piper_rl_mujoco import MujocoRobotEnv

        print("ğŸš€ ä½¿ç”¨Mujocoä»¿çœŸ")

    if args.test:
        test()
    else:
        train()
