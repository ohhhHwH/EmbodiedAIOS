import os
import numpy as np
from gymnasium.wrappers import RecordVideo
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
import time
from piper_rl import RobotEnv


def decay_schedule(initial_value):
    def func(progress_remaining):
        return initial_value * (progress_remaining)

    return func


def make_env(MyRobotEnv, ctrl_mode, worker_id):
    def _init():
        env = MyRobotEnv(ctrl_mode=ctrl_mode, worker_id=worker_id)
        return env

    return _init


def train():
    print("ğŸš€ è¿›ç¨‹æ•°:", args.proc)
    if args.proc > 1:
        if not args.ctrl_mode == "mujoco":
            raise ValueError("å¤šè¿›ç¨‹ä»¿çœŸåªæ”¯æŒ mujoco æ¨¡å¼")
        if args.record:
            raise ValueError("å¤šè¿›ç¨‹ä»¿çœŸä¸æ”¯æŒå½•åˆ¶è§†é¢‘ï¼Œè¯·è®¾ç½® --record False")
        from stable_baselines3.common.vec_env import SubprocVecEnv

        env = SubprocVecEnv(
            [make_env(RobotEnv, args.ctrl_mode, i) for i in range(args.proc)],
            start_method="spawn",
        )
    else:
        if args.record:
            # éœ€è¦ export MUJOCO_GL=egl
            os.environ["MUJOCO_GL"] = "egl"
            env = RobotEnv(ctrl_mode=args.ctrl_mode, render_mode="rgb_array")
            video_dir = "./videos/"
            env = RecordVideo(
                env,
                video_folder=video_dir,
                episode_trigger=lambda e: e % 100 == 0,
                video_length=5000,
            )
        else:
            env = RobotEnv(ctrl_mode=args.ctrl_mode)
    model = PPO(
        policy="MlpPolicy",
        env=env,
        device="cpu",
        policy_kwargs=dict(
            net_arch=[256, 512, 512, 128],
            log_std_init=-2.0,
            ortho_init=True,
        ),
        # åœ¨å·²è®­ç»ƒå¥½çš„æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒï¼Œéœ€è¦è°ƒå°å­¦ä¹ ç‡ï¼Œå¦‚3e-5
        learning_rate=decay_schedule(3e-4),
        batch_size=1024,
        n_steps=2048,
        gamma=0.99,
        verbose=1,
        # è¶Šé«˜è¶Šé¼“åŠ±æ¢ç´¢ï¼Œä½†ä¸èƒ½å¤ªé«˜ï¼Œå¦åˆ™stdä¼šå˜å¤§ï¼Œç­–ç•¥ä¼šå˜å¾—ä¸ç¨³å®šï¼Œä¸€èˆ¬1e-2ä»¥å†…
        ent_coef=1e-2,
        tensorboard_log="./ppo_logs/",
    )
    # model.set_parameters("ppo_piper_final.zip")

    checkpoint_callback = CheckpointCallback(
        save_freq=100000, save_path="./ppo_models/", name_prefix="piper_rl_checkpoint"
    )

    model.learn(total_timesteps=1000 * 1000 * 100, callback=checkpoint_callback)

    model.save("ppo_piper_final")
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜ä¸º ppo_piper_final.zip")


def test():
    if args.record:
        env = RobotEnv(ctrl_mode=args.ctrl_mode, render_mode="rgb_array")
        video_dir = "./videos/"
        env = RecordVideo(
            env,
            video_folder=video_dir,
            episode_trigger=lambda e: e % 1 == 0,
            video_length=5000,
        )
    else:
        env = RobotEnv(ctrl_mode=args.ctrl_mode)
    model = PPO.load("ppo_piper_final")
    obs = env.reset()[0]
    for epoch in range(100000):
        action, _ = model.predict(obs)
        obs, reward, done, _, _ = env.step(action)

        print(f"epoch {epoch}, Reward:", reward)
        if done:
            print("ğŸ‰ æˆåŠŸæŠ“å–ï¼Œé‡æ–°å¼€å§‹")
            obs = env.reset()
        time.sleep(0.5)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•è®­ç»ƒå¥½çš„ç­–ç•¥")
    parser.add_argument(
        "--ctrl_mode",
        choices=["mujoco", "ros", "piper_sdk"],
        default="mujoco",
        help="æ§åˆ¶æ¨¡å¼ï¼Œä½¿ç”¨mujocoä»¿çœŸ/ros",
    )
    parser.add_argument(
        "--record",
        default=False,
        action="store_true",
        help="å½•åˆ¶è§†é¢‘ï¼Œé»˜è®¤True",
    )
    parser.add_argument("--proc", default=64, help="å¹¶è¡Œä»¿çœŸè¿›ç¨‹æ•°")
    args = parser.parse_args()
    if args.ctrl_mode == "ros":
        print("ğŸš€ ä½¿ç”¨rosæ§åˆ¶")
    else:
        print("ğŸš€ ä½¿ç”¨Mujocoä»¿çœŸ")

    if args.test:
        test()
    else:
        train()
